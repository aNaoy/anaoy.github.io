---
title: 'GitHub Copilot: Remote Code Execution via Prompt Injection (CVE-2025-53773)'
date: 2025-08-13
permalink: /posts/2025/08/13/github-copilot-remote-code-execution-via-prompt-injection-cve-2025-53773/
tags:
- veille-cyber
- zerodaysfans
---
### Exécution de Code à Distance via Injection de Prompt dans GitHub Copilot

Une faille de sécurité critique, identifiée par la **CVE-2025-53773**, permettait l'exécution de code à distance sur la machine d'un développeur utilisant GitHub Copilot dans VS Code.

#### Points Clés :
*   La vulnérabilité exploitait une fonctionnalité expérimentale de VS Code permettant à GitHub Copilot de modifier directement les fichiers de configuration du projet.
*   En ajoutant une ligne spécifique au fichier `.vscode/settings.json`, Copilot pouvait être mis en mode "YOLO" (You Only Live Once).
*   Ce mode désactivait toutes les confirmations utilisateur, permettant à l'IA d'exécuter des commandes système, de naviguer sur le web, et d'autres actions.
*   L'exploitation commençait par une injection de prompt dans un fichier source, une issue GitHub, ou une autre source de données.
*   L'injection modifiait le fichier `settings.json` pour activer le mode "YOLO".
*   Une fois en mode "YOLO", des commandes système pouvaient être exécutées, potentiellement ciblant le système d'exploitation spécifique de la machine.
*   Cela ouvrait la voie à la création de "ZombAI", transformant la machine du développeur en botnet, au vol d'informations, ou au déploiement de rançongiciels.
*   La technique pouvait également être utilisée pour créer des "virus IA" qui se propageaient via des projets Git contaminés.
*   Des instructions cachées (en utilisant des caractères Unicode invisibles) pouvaient également être employées pour masquer l'attaque, bien que cela puisse rendre l'exploitation moins fiable.

#### Vulnérabilité :
*   **CVE-2025-53773** : Exécution de code à distance dans GitHub Copilot et VS Code due à une injection de prompt permettant la modification de la configuration.

#### Recommandations :
*   Idéalement, les agents IA ne devraient pas pouvoir modifier les fichiers sans approbation humaine préalable. De nombreux éditeurs montrent un aperçu des modifications qui doit ensuite être validé par le développeur.
*   Il est crucial de réaliser une analyse des menaces (threat modeling) pour identifier de telles failles de conception dans les systèmes d'agents.
*   La désactivation de fonctionnalités expérimentales non nécessaires peut réduire la surface d'attaque.
*   Éviter d'incorporer des instructions non visibles ou non évidentes dans les prompts ou les fichiers de projet.

Microsoft a corrigé cette vulnérabilité dans la mise à jour d'août 2025. La découverte a été rendue publique suite à un processus de divulgation responsable, avec une contribution également rapportée par Markus Vervier de Persistent Security.

---
[Source](https://embracethered.com/blog/posts/2025/github-copilot-remote-code-execution-via-prompt-injection/){:target="_blank"}
